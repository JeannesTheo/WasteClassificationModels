{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2q27gKz1H20"
   },
   "source": [
    "##### Copyright 2021 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr3q-gvm3cI8"
   },
   "source": [
    "The Model Maker library uses *transfer learning* to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with your own custom dataset reduces the amount of training data required and will shorten the training time.\n",
    "\n",
    "\n",
    "This tutorial uses the EfficientDet-Lite0 model. EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "\n",
    "Trust the 'requirements.txt' file to install the correct versions of the packages, and install them in one go :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !pip install roboflow\n",
    "# !pip uninstall -y tensorflow && pip install -q tensorflow==2.8.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T15:48:26.806864881Z",
     "start_time": "2024-01-08T15:48:26.757010972Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6lRhVK9Q_0U"
   },
   "source": [
    "Import the required packages. Those marked as unused are still necessary to make the Model Maker library work."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 16:48:28.430549: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/theojeannes/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-01-08 16:48:28.430567: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from roboflow import Roboflow\n",
    "import os\n",
    "\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "import pycocotools as pycocotools\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "MODEL_NAME = \"efficientdet_lite4\"\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T15:48:32.770120306Z",
     "start_time": "2024-01-08T15:48:26.812475142Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRd13bfetO7B"
   },
   "source": [
    "### Prepare the dataset\n",
    "\n",
    "We need to have a dataset cleaned and ready to be used.\n",
    "\n",
    "The dataset is provided in CSV format:\n",
    "```\n",
    "TRAINING,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Salad,0.0,0.0954,,,0.977,0.957,,\n",
    "VALIDATION,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Seafood,0.0154,0.1538,,,1.0,0.802,,\n",
    "TEST,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Tomato,0.0,0.655,,,0.231,0.839,,\n",
    "```\n",
    "\n",
    "* Each row corresponds to an object localized inside a larger image, with each object specifically designated as test, train, or validation data. You'll learn more about what that means in a later stage in this notebook.\n",
    "* The three lines included here indicate **three distinct objects located inside the same image** available at `gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg`.\n",
    "* Each row has a different label: `Salad`, `Seafood`, `Tomato`, etc.\n",
    "* Bounding boxes are specified for each image using the top left and bottom right vertices.\n",
    "\n",
    "If you want to know more about how to prepare your own CSV file and the minimum requirements for creating a valid dataset, see the [Preparing your training data](https://cloud.google.com/vision/automl/object-detection/docs/prepare) guide for more details.\n",
    "\n",
    "If you are new to Google Cloud, you may wonder what the `gs://` URL means. They are URLs of files stored on [Google Cloud Storage](https://cloud.google.com/storage) (GCS). If you make your files on GCS public or [authenticate your client](https://cloud.google.com/storage/docs/authentication#libauth), Model Maker can read those files similarly to your local files.\n",
    "\n",
    "However, you don't need to keep your images on Google Cloud to use Model Maker. You can use a local path in your CSV file and Model Maker will just work.\n",
    "\n",
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CtdZ-JDwMimd",
    "ExecuteTime": {
     "end_time": "2024-01-08T15:48:32.783697800Z",
     "start_time": "2024-01-08T15:48:32.781664787Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 16:48:32.773740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/theojeannes/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-01-08 16:48:32.773767: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-01-08 16:48:32.773787: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (theojeannes): /proc/driver/nvidia/version does not exist\n",
      "2024-01-08 16:48:32.775657: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "spec = model_spec.get(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5U-A3tw6Y27"
   },
   "source": [
    "Import**Step 2. Load the dataset.**\n",
    "\n",
    "Model Maker will take input data in the CSV format. Use the `object_detector.DataLoader.from_csv` method to load the dataset and split them into the training, validation and test images.\n",
    "\n",
    "* Training images: These images are used to train the object detection model to recognize salad ingredients.\n",
    "* Validation images: These are images that the model didn't see during the training process. You'll use them to decide when you should stop the training, to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "* Test images: These images are used to evaluate the final model performance.\n",
    "\n",
    "You can load the CSV file directly from Google Cloud Storage, but you don't need to keep your images on Google Cloud to use Model Maker. You can specify a local CSV file on your computer, and Model Maker will work just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HD5BvzWe6YKa",
    "ExecuteTime": {
     "end_time": "2024-01-08T15:48:32.888373801Z",
     "start_time": "2024-01-08T15:48:32.783821202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader.DataLoader object at 0x73c72386c4c0> <tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader.DataLoader object at 0x73c6cb4dc310> <tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader.DataLoader object at 0x73c6cb4dc550>\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data = object_detector.DataLoader.from_csv(\n",
    "    \"data/annotations.csv\",\n",
    "    images_dir=\"data\")\n",
    "print(train_data,validation_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uZkLR6N6gDR"
   },
   "source": [
    "**Step 3. Train the TensorFlow model with the training data.**\n",
    "\n",
    "* `epochs = 50` by default, which means it will go through the training dataset 50 times. You can look at the validation accuracy during training and stop early to avoid overfitting.\n",
    "* Set `batch_size = 8`, size of one batch (how many rough the 175 images in the training dataset.\n",
    "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kwlYdTcg63xy",
    "ExecuteTime": {
     "end_time": "2024-01-08T15:54:50.880245899Z",
     "start_time": "2024-01-08T15:48:32.890020578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 16:48:45.315457: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2024-01-08 16:49:00.785885: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3774873600 exceeds 10% of free system memory.\n",
      "2024-01-08 16:49:01.170839: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3774873600 exceeds 10% of free system memory.\n",
      "2024-01-08 16:49:05.874889: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 943718400 exceeds 10% of free system memory.\n",
      "2024-01-08 16:49:06.475476: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 943718400 exceeds 10% of free system memory.\n",
      "2024-01-08 16:49:07.104352: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1258291200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 89s 89s/step - det_loss: 1.8345 - cls_loss: 1.1894 - box_loss: 0.0129 - reg_l2_loss: 0.0075 - loss: 1.8420 - learning_rate: 0.0080 - gradient_norm: 0.3674\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 71s 71s/step - det_loss: 1.8613 - cls_loss: 1.1916 - box_loss: 0.0134 - reg_l2_loss: 0.0075 - loss: 1.8688 - learning_rate: 0.0683 - gradient_norm: 0.3644\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 66s 66s/step - det_loss: 1.8186 - cls_loss: 1.1805 - box_loss: 0.0128 - reg_l2_loss: 0.0075 - loss: 1.8261 - learning_rate: 0.0400 - gradient_norm: 0.3122\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 68s 68s/step - det_loss: 1.8068 - cls_loss: 1.1770 - box_loss: 0.0126 - reg_l2_loss: 0.0075 - loss: 1.8143 - learning_rate: 0.0117 - gradient_norm: 0.3030\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 71s 71s/step - det_loss: 1.7988 - cls_loss: 1.1789 - box_loss: 0.0124 - reg_l2_loss: 0.0075 - loss: 1.8062 - learning_rate: 0.0000e+00 - gradient_norm: 0.3029\n"
     ]
    }
   ],
   "source": [
    "model = object_detector.create(train_data, model_spec=spec, batch_size=64, train_whole_model=False, validation_data=validation_data,epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BzCHLWJ6h7q"
   },
   "source": [
    "**Step 4. Evaluate the model with the test data.**\n",
    "\n",
    "We are mainly interested in the AP75 (which is the mAP with IoU threshold 0.75) and ARmax10(Average Recall, and we use the top 10 confidence threshold to say if a result is a true positive or a false positive). The higher the better. Here we are evaluating on the model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metrics = ['AP75','APs','APm','APl','ARmax10','ARs','ARm','ARl']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T15:54:50.885492731Z",
     "start_time": "2024-01-08T15:54:50.877417357Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 16:54:51.670972: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 18s 18s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "         Original efficientdet_lite4\nMetric                              \nAP75                        0.000000\nAPs                         0.000000\nAPm                         0.000000\nAPl                         0.005223\nARmax10                     0.012529\nARs                         0.000000\nARm                         0.000000\nARl                         0.035471",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Original efficientdet_lite4</th>\n    </tr>\n    <tr>\n      <th>Metric</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AP75</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>APs</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>APm</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>APl</th>\n      <td>0.005223</td>\n    </tr>\n    <tr>\n      <th>ARmax10</th>\n      <td>0.012529</td>\n    </tr>\n    <tr>\n      <th>ARs</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>ARm</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>ARl</th>\n      <td>0.035471</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.evaluate(test_data)\n",
    "data = {m: res[m] for m in metrics}\n",
    "df = pd.DataFrame(list(data.items()), columns=['Metric', 'Original '+MODEL_NAME])\n",
    "\n",
    "# Set 'Metric' as the index\n",
    "df.set_index('Metric', inplace=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T15:55:09.216895948Z",
     "start_time": "2024-01-08T15:54:50.879216091Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgCDMe0e6jlT"
   },
   "source": [
    "**Step 5.  Export as a TensorFlow Lite model.**\n",
    "\n",
    "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is full integer quantization.\n",
    "\n",
    "Several factors can affect the model accuracy when exporting to TFLite:\n",
    "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
    "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
    "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
    "\n",
    "Therefore, you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "quantization_configs = {\n",
    "    'vanilla': None,\n",
    "    'float16': QuantizationConfig.for_float16(),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T15:55:09.217468235Z",
     "start_time": "2024-01-08T15:55:09.211086568Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 16:55:10.903680: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2024-01-08 16:55:41.310601: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'resample_p7/PartitionedCall' has 1 outputs but the _output_shapes attribute specifies shapes for 3 outputs. Output shapes may be inaccurate.\n",
      "2024-01-08 16:55:49.510155: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2024-01-08 16:55:49.510211: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2024-01-08 16:55:49.514777: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpkzcgyxzf\n",
      "2024-01-08 16:55:49.647490: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2024-01-08 16:55:49.647537: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpkzcgyxzf\n",
      "2024-01-08 16:55:50.144943: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2024-01-08 16:55:52.142060: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpkzcgyxzf\n",
      "2024-01-08 16:55:53.172595: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 3658598 microseconds.\n",
      "2024-01-08 16:55:55.241853: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-08 16:55:57.534561: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 37.694 G  ops, equivalently 18.847 G  MACs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modele exporté :  efficientdet_lite4_vanilla.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 31s 874ms/step\n",
      "\n",
      "Evaluation du modèle {'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0052365796, 'ARmax10': 0.020683728, 'ARs': 0.0, 'ARm': 0.0, 'ARl': 0.03636696}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 16:57:00.959946: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'resample_p7/PartitionedCall' has 1 outputs but the _output_shapes attribute specifies shapes for 3 outputs. Output shapes may be inaccurate.\n",
      "2024-01-08 16:57:09.221215: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2024-01-08 16:57:09.221245: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2024-01-08 16:57:09.221419: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpfgq3z8ws\n",
      "2024-01-08 16:57:09.301641: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2024-01-08 16:57:09.301686: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpfgq3z8ws\n",
      "2024-01-08 16:57:09.639248: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2024-01-08 16:57:11.709017: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpfgq3z8ws\n",
      "2024-01-08 16:57:12.794813: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 3573395 microseconds.\n",
      "2024-01-08 16:57:17.085505: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 37.694 G  ops, equivalently 18.847 G  MACs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modele exporté :  efficientdet_lite4_float16.tflite\n",
      "35/35 [==============================] - 30s 852ms/step\n",
      "\n",
      "Evaluation du modèle {'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.005206832, 'ARmax10': 0.020683728, 'ARs': 0.0, 'ARm': 0.0, 'ARl': 0.03636696}\n"
     ]
    }
   ],
   "source": [
    "for k,v in quantization_configs.items():\n",
    "    config_name = MODEL_NAME+\"_\"+k+\".tflite\"\n",
    "    model.export(export_dir='.',tflite_filename=config_name ,quantization_config=v)\n",
    "    print(\"Modele exporté : \",config_name)\n",
    "    res = model.evaluate_tflite(config_name, test_data)\n",
    "    res_filtered = {m: res[m] for m in metrics}\n",
    "    print(\"Evaluation du modèle\",res_filtered)\n",
    "    df[config_name.split('.')[0]] = pd.Series(res_filtered)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T15:57:49.785170951Z",
     "start_time": "2024-01-08T15:55:09.214142087Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         Original efficientdet_lite4  efficientdet_lite4_vanilla  \\\nMetric                                                             \nAP75                        0.000000                    0.000000   \nAPs                         0.000000                    0.000000   \nAPm                         0.000000                    0.000000   \nAPl                         0.005223                    0.005237   \nARmax10                     0.012529                    0.020684   \nARs                         0.000000                    0.000000   \nARm                         0.000000                    0.000000   \nARl                         0.035471                    0.036367   \n\n         efficientdet_lite4_float16  \nMetric                               \nAP75                       0.000000  \nAPs                        0.000000  \nAPm                        0.000000  \nAPl                        0.005207  \nARmax10                    0.020684  \nARs                        0.000000  \nARm                        0.000000  \nARl                        0.036367  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Original efficientdet_lite4</th>\n      <th>efficientdet_lite4_vanilla</th>\n      <th>efficientdet_lite4_float16</th>\n    </tr>\n    <tr>\n      <th>Metric</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AP75</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>APs</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>APm</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>APl</th>\n      <td>0.005223</td>\n      <td>0.005237</td>\n      <td>0.005207</td>\n    </tr>\n    <tr>\n      <th>ARmax10</th>\n      <td>0.012529</td>\n      <td>0.020684</td>\n      <td>0.020684</td>\n    </tr>\n    <tr>\n      <th>ARs</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>ARm</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>ARl</th>\n      <td>0.035471</td>\n      <td>0.036367</td>\n      <td>0.036367</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T15:57:49.793012469Z",
     "start_time": "2024-01-08T15:57:49.789814194Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me6_RwPZqNhX"
   },
   "source": [
    "## Test the TFLite model on your image\n",
    "\n",
    "Use the app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8VxPiOLy4Gv"
   },
   "source": [
    "### Customize the EfficientDet model hyperparameters\n",
    "\n",
    "The model and training pipeline parameters you can adjust are:\n",
    "\n",
    "* `model_dir`: The location to save the model checkpoint files. If not set, a temporary directory will be used.\n",
    "* `steps_per_execution`: Number of steps per training execution.\n",
    "* `moving_average_decay`: Float. The decay to use for maintaining moving averages of the trained parameters.\n",
    "* `var_freeze_expr`: The regular expression to map the prefix name of variables to be frozen which means remaining the same during training. More specific, use `re.match(var_freeze_expr, variable_name)` in the codebase to map the variables to be frozen.\n",
    "* `tflite_max_detections`: integer, 25 by default. The max number of output detections in the TFLite model.\n",
    "* `strategy`:  A string specifying which distribution strategy to use. Accepted values are 'tpu', 'gpus', None. tpu' means to use TPUStrategy. 'gpus' mean to use MirroredStrategy for multi-gpus. If None, use TF default with OneDeviceStrategy.\n",
    "* `tpu`:  The Cloud TPU to use for training. This should be either the name used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\n",
    "* `use_xla`: Use XLA even if strategy is not tpu. If strategy is tpu, always use XLA, and this flag has no effect.\n",
    "* `profile`: Enable profile mode.\n",
    "* `debug`: Enable debug mode.\n",
    "\n",
    "Other parameters that can be adjusted is shown in [hparams_config.py](https://github.com/google/automl/blob/df451765d467c5ed78bbdfd632810bc1014b123e/efficientdet/hparams_config.py#L170).\n",
    "\n",
    "\n",
    "For instance, you can set the `var_freeze_expr='efficientnet'` which freezes the variables with name prefix `efficientnet` (default is `'(efficientnet|fpn_cells|resample_p6)'`). This allows the model to freeze untrainable variables and keep their value the same through training.\n",
    "\n",
    "```python\n",
    "spec = model_spec.get('efficientdet_lite0')\n",
    "spec.config.var_freeze_expr = 'efficientnet'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvQuy7RSDir3"
   },
   "source": [
    "### Tune the training hyperparameters\n",
    "\n",
    "The `create` function is the driver function that the Model Maker library uses to create models. The `model_spec` parameter defines the model specification. The `object_detector.EfficientDetSpec` class is currently supported. The `create` function comprises of the following steps:\n",
    "\n",
    "1. Creates the model for the object detection according to `model_spec`.\n",
    "2. Trains the model.  The default epochs and the default batch size are set by the `epochs` and `batch_size` variables in the `model_spec` object.\n",
    "You can also tune the training hyperparameters like `epochs` and `batch_size` that affect the model accuracy. For instance,\n",
    "\n",
    "*   `epochs`: Integer, 50 by default. More epochs could achieve better accuracy, but may lead to overfitting.\n",
    "*   `batch_size`: Integer, 64 by default. The number of samples to use in one training step.\n",
    "*   `train_whole_model`: Boolean, False by default. If true, train the whole model. Otherwise, only train the layers that do not match `var_freeze_expr`.\n",
    "\n",
    "For example, you can train with less epochs and only the head layer. You can increase the number of epochs for better results.\n",
    "\n",
    "```python\n",
    "model = object_detector.create(train_data, model_spec=spec, epochs=10, validation_data=validation_data)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model Maker Object Detection Tutorial",
   "provenance": [
    {
     "file_id": "1dbRXQCjtm-jBFC32DJ6YCVXnXBOG3M5t",
     "timestamp": 1613441434239
    },
    {
     "file_id": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb",
     "timestamp": 1612303859066
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
