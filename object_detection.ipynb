{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2q27gKz1H20"
   },
   "source": [
    "##### Copyright 2021 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr3q-gvm3cI8"
   },
   "source": [
    "The Model Maker library uses *transfer learning* to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with your own custom dataset reduces the amount of training data required and will shorten the training time.\n",
    "\n",
    "\n",
    "This tutorial uses the EfficientDet-Lite0 model. EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "\n",
    "Trust the 'requirements.txt' file to install the correct versions of the packages, and install them in one go :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !pip install roboflow\n",
    "# !pip uninstall -y tensorflow && pip install -q tensorflow==2.8.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:59:08.428725191Z",
     "start_time": "2024-01-07T12:59:08.382343687Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6lRhVK9Q_0U"
   },
   "source": [
    "Import the required packages. Those marked as unused are still necessary to make the Model Maker library work."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from roboflow import Roboflow\n",
    "import os\n",
    "\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "import pycocotools as pycocotools\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "MODEL_NAME = \"efficientdet_lite4\"\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:59:08.460233937Z",
     "start_time": "2024-01-07T12:59:08.435275632Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRd13bfetO7B"
   },
   "source": [
    "### Prepare the dataset\n",
    "\n",
    "We need to have a dataset cleaned and ready to be used.\n",
    "\n",
    "The dataset is provided in CSV format:\n",
    "```\n",
    "TRAINING,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Salad,0.0,0.0954,,,0.977,0.957,,\n",
    "VALIDATION,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Seafood,0.0154,0.1538,,,1.0,0.802,,\n",
    "TEST,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,Tomato,0.0,0.655,,,0.231,0.839,,\n",
    "```\n",
    "\n",
    "* Each row corresponds to an object localized inside a larger image, with each object specifically designated as test, train, or validation data. You'll learn more about what that means in a later stage in this notebook.\n",
    "* The three lines included here indicate **three distinct objects located inside the same image** available at `gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg`.\n",
    "* Each row has a different label: `Salad`, `Seafood`, `Tomato`, etc.\n",
    "* Bounding boxes are specified for each image using the top left and bottom right vertices.\n",
    "\n",
    "If you want to know more about how to prepare your own CSV file and the minimum requirements for creating a valid dataset, see the [Preparing your training data](https://cloud.google.com/vision/automl/object-detection/docs/prepare) guide for more details.\n",
    "\n",
    "If you are new to Google Cloud, you may wonder what the `gs://` URL means. They are URLs of files stored on [Google Cloud Storage](https://cloud.google.com/storage) (GCS). If you make your files on GCS public or [authenticate your client](https://cloud.google.com/storage/docs/authentication#libauth), Model Maker can read those files similarly to your local files.\n",
    "\n",
    "However, you don't need to keep your images on Google Cloud to use Model Maker. You can use a local path in your CSV file and Model Maker will just work.\n",
    "\n",
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CtdZ-JDwMimd",
    "ExecuteTime": {
     "end_time": "2024-01-07T12:59:08.504117157Z",
     "start_time": "2024-01-07T12:59:08.466102685Z"
    }
   },
   "outputs": [],
   "source": [
    "spec = model_spec.get(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5U-A3tw6Y27"
   },
   "source": [
    "Import**Step 2. Load the dataset.**\n",
    "\n",
    "Model Maker will take input data in the CSV format. Use the `object_detector.DataLoader.from_csv` method to load the dataset and split them into the training, validation and test images.\n",
    "\n",
    "* Training images: These images are used to train the object detection model to recognize salad ingredients.\n",
    "* Validation images: These are images that the model didn't see during the training process. You'll use them to decide when you should stop the training, to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "* Test images: These images are used to evaluate the final model performance.\n",
    "\n",
    "You can load the CSV file directly from Google Cloud Storage, but you don't need to keep your images on Google Cloud to use Model Maker. You can specify a local CSV file on your computer, and Model Maker will work just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HD5BvzWe6YKa",
    "ExecuteTime": {
     "end_time": "2024-01-07T12:59:08.549475415Z",
     "start_time": "2024-01-07T12:59:08.504325129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader.DataLoader object at 0x7176c8db2160> <tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader.DataLoader object at 0x717794647430> <tensorflow_examples.lite.model_maker.core.data_util.object_detector_dataloader.DataLoader object at 0x7176d00bd640>\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data = object_detector.DataLoader.from_csv(\n",
    "    \"data/annotations.csv\",\n",
    "    images_dir=\"data\",\n",
    "    cache_dir=\"cache\")\n",
    "print(train_data,validation_data, test_data)\n",
    "# from_csv(\n",
    "#     cls,\n",
    "#     filename: str,\n",
    "# images_dir: Optional[str] = None,\n",
    "# delimiter: str = ',',\n",
    "# quotechar: str = '\"',\n",
    "# num_shards: int = 10,\n",
    "# max_num_images: Optional[int] = None,\n",
    "# cache_dir: Optional[str] = None,\n",
    "# cache_prefix_filename: Optional[str] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uZkLR6N6gDR"
   },
   "source": [
    "**Step 3. Train the TensorFlow model with the training data.**\n",
    "\n",
    "* `epochs = 50` by default, which means it will go through the training dataset 50 times. You can look at the validation accuracy during training and stop early to avoid overfitting.\n",
    "* Set `batch_size = 8`, size of one batch (how many rough the 175 images in the training dataset.\n",
    "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kwlYdTcg63xy",
    "ExecuteTime": {
     "end_time": "2024-01-07T13:00:22.848775207Z",
     "start_time": "2024-01-07T12:59:08.549205918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 13:59:23.949146: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 49s 49s/step - det_loss: 1.9293 - cls_loss: 1.2429 - box_loss: 0.0137 - reg_l2_loss: 0.1080 - loss: 2.0372 - learning_rate: 0.0080 - gradient_norm: 4.2444\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 5s 5s/step - det_loss: 1.8723 - cls_loss: 1.2100 - box_loss: 0.0132 - reg_l2_loss: 0.1080 - loss: 1.9802 - learning_rate: 0.0012 - gradient_norm: 3.5213\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 5s 5s/step - det_loss: 1.8189 - cls_loss: 1.1683 - box_loss: 0.0130 - reg_l2_loss: 0.1080 - loss: 1.9268 - learning_rate: 0.0000e+00 - gradient_norm: 2.9331\n"
     ]
    }
   ],
   "source": [
    "model = object_detector.create(train_data, model_spec=spec, batch_size=2, train_whole_model=True, validation_data=validation_data,epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BzCHLWJ6h7q"
   },
   "source": [
    "**Step 4. Evaluate the model with the test data.**\n",
    "\n",
    "We are mainly interested in the AP75 (which is the mAP with IoU threshold 0.75) and ARmax10(Average Recall, and we use the top 10 confidence threshold to say if a result is a true positive or a false positive). The higher the better. Here we are evaluating on the model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metrics = ['AP75','APs','APm','APl','ARmax10','ARs','ARm','ARl']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T13:00:22.857842697Z",
     "start_time": "2024-01-07T13:00:22.854102876Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 14:00:23.608719: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "         Vanilla\nMetric          \nAP75         0.0\nAPs         -1.0\nAPm         -1.0\nAPl          0.0\nARmax10      0.0\nARs         -1.0\nARm         -1.0\nARl          0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Vanilla</th>\n    </tr>\n    <tr>\n      <th>Metric</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AP75</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>APs</th>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>APm</th>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>APl</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ARmax10</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ARs</th>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>ARm</th>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>ARl</th>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.evaluate(test_data)\n",
    "data = {m: res[m] for m in metrics}\n",
    "df = pd.DataFrame(list(data.items()), columns=['Metric', 'Vanilla'])\n",
    "\n",
    "# Set 'Metric' as the index\n",
    "df.set_index('Metric', inplace=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T13:00:28.908180651Z",
     "start_time": "2024-01-07T13:00:22.858640779Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgCDMe0e6jlT"
   },
   "source": [
    "**Step 5.  Export as a TensorFlow Lite model.**\n",
    "\n",
    "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is full integer quantization.\n",
    "\n",
    "Several factors can affect the model accuracy when exporting to TFLite:\n",
    "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
    "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
    "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
    "\n",
    "Therefore, you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "quantization_configs = {\n",
    "    'vanilla': None,\n",
    "    'float16': QuantizationConfig.for_float16()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T13:00:41.819810652Z",
     "start_time": "2024-01-07T13:00:41.757607804Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k,v \u001B[38;5;129;01min\u001B[39;00m quantization_configs\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m      2\u001B[0m     config_name \u001B[38;5;241m=\u001B[39m MODEL_NAME\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39mk\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.tflite\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexport_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mtflite_filename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig_name\u001B[49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     res[k] \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate_tflite(config_name, test_data)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(res)\n",
      "File \u001B[0;32m~/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/custom_model.py:132\u001B[0m, in \u001B[0;36mCustomModel.export\u001B[0;34m(self, export_dir, tflite_filename, label_filename, vocab_filename, saved_model_filename, tfjs_folder_name, export_format, **kwargs)\u001B[0m\n\u001B[1;32m    130\u001B[0m   tflite_filepath \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(export_dir, tflite_filename)\n\u001B[1;32m    131\u001B[0m   export_tflite_kwargs, kwargs \u001B[38;5;241m=\u001B[39m _get_params(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_export_tflite, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 132\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_export_tflite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtflite_filepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mexport_tflite_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m   tf\u001B[38;5;241m.\u001B[39mcompat\u001B[38;5;241m.\u001B[39mv1\u001B[38;5;241m.\u001B[39mlogging\u001B[38;5;241m.\u001B[39minfo(\n\u001B[1;32m    134\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTensorFlow Lite model exported successfully: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m tflite_filepath)\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py:185\u001B[0m, in \u001B[0;36mObjectDetector._export_tflite\u001B[0;34m(self, tflite_filepath, quantization_config, with_metadata, export_metadata_json_file)\u001B[0m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m quantization_config \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    182\u001B[0m   quantization_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_spec\u001B[38;5;241m.\u001B[39mget_default_quantization_config(\n\u001B[1;32m    183\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepresentative_data)\n\u001B[0;32m--> 185\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_spec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport_tflite\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtflite_filepath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_metadata:\n\u001B[1;32m    189\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m tempfile\u001B[38;5;241m.\u001B[39mTemporaryDirectory() \u001B[38;5;28;01mas\u001B[39;00m temp_dir:\n",
      "File \u001B[0;32m~/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py:497\u001B[0m, in \u001B[0;36mEfficientDetModelSpec.export_tflite\u001B[0;34m(self, model, tflite_filepath, quantization_config)\u001B[0m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tempfile\u001B[38;5;241m.\u001B[39mTemporaryDirectory() \u001B[38;5;28;01mas\u001B[39;00m temp_dir:\n\u001B[1;32m    495\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexport_saved_model(\n\u001B[1;32m    496\u001B[0m       model, temp_dir, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, pre_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, post_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtflite\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 497\u001B[0m   converter \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFLiteConverter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_saved_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtemp_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    499\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m quantization_config:\n\u001B[1;32m    500\u001B[0m     converter \u001B[38;5;241m=\u001B[39m quantization_config\u001B[38;5;241m.\u001B[39mget_converter_with_quantization(\n\u001B[1;32m    501\u001B[0m         converter, model_spec\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1648\u001B[0m, in \u001B[0;36mTFLiteConverterV2.from_saved_model\u001B[0;34m(cls, saved_model_dir, signature_keys, tags)\u001B[0m\n\u001B[1;32m   1645\u001B[0m   tags \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m([_tag_constants\u001B[38;5;241m.\u001B[39mSERVING])\n\u001B[1;32m   1647\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context\u001B[38;5;241m.\u001B[39meager_mode():\n\u001B[0;32m-> 1648\u001B[0m   saved_model \u001B[38;5;241m=\u001B[39m \u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43msaved_model_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1649\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signature_keys:\n\u001B[1;32m   1650\u001B[0m   signature_keys \u001B[38;5;241m=\u001B[39m saved_model\u001B[38;5;241m.\u001B[39msignatures\n",
      "File \u001B[0;32m~/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:936\u001B[0m, in \u001B[0;36mload\u001B[0;34m(export_dir, tags, options)\u001B[0m\n\u001B[1;32m    845\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msaved_model.load\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msaved_model.load_v2\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    846\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(export_dir, tags\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    847\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Load a SavedModel from `export_dir`.\u001B[39;00m\n\u001B[1;32m    848\u001B[0m \n\u001B[1;32m    849\u001B[0m \u001B[38;5;124;03m  Signatures associated with the SavedModel are available as functions:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    934\u001B[0m \u001B[38;5;124;03m    ValueError: If `tags` don't match a MetaGraph in the SavedModel.\u001B[39;00m\n\u001B[1;32m    935\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 936\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[43mload_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexport_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroot\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    937\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:974\u001B[0m, in \u001B[0;36mload_internal\u001B[0;34m(export_dir, tags, options, loader_cls, filters)\u001B[0m\n\u001B[1;32m    972\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39minit_scope():\n\u001B[1;32m    973\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 974\u001B[0m     loader \u001B[38;5;241m=\u001B[39m \u001B[43mloader_cls\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobject_graph_proto\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msaved_model_proto\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexport_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    975\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mckpt_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilters\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    976\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mNotFoundError \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    977\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[1;32m    978\u001B[0m         \u001B[38;5;28mstr\u001B[39m(err) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m You may be trying to load on a different device \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    979\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom the computational device. Consider setting the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    980\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    981\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto the io_device such as \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/job:localhost\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:149\u001B[0m, in \u001B[0;36mLoader.__init__\u001B[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_proto \u001B[38;5;241m=\u001B[39m object_graph_proto\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_export_dir \u001B[38;5;241m=\u001B[39m export_dir\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_concrete_functions \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 149\u001B[0m     \u001B[43mfunction_deserialization\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_function_def_library\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlibrary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmeta_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_def\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlibrary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m        \u001B[49m\u001B[43msaved_object_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_proto\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwrapper_function\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_WrapperFunction\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;66;03m# Store a set of all concrete functions that have been set up with\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;66;03m# captures.\u001B[39;00m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restored_concrete_functions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/tensorflow/python/saved_model/function_deserialization.py:406\u001B[0m, in \u001B[0;36mload_function_def_library\u001B[0;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001B[0m\n\u001B[1;32m    400\u001B[0m \u001B[38;5;66;03m# There is no need to copy all functions into the function def graph. It\u001B[39;00m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;66;03m# leads to a O(n^2) increase of memory when importing functions and the\u001B[39;00m\n\u001B[1;32m    402\u001B[0m \u001B[38;5;66;03m# extra function definitions are a no-op since they already imported as a\u001B[39;00m\n\u001B[1;32m    403\u001B[0m \u001B[38;5;66;03m# function before and passed in explicitly (due to the topologic sort\u001B[39;00m\n\u001B[1;32m    404\u001B[0m \u001B[38;5;66;03m# import).\u001B[39;00m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m graph\u001B[38;5;241m.\u001B[39mas_default():\n\u001B[0;32m--> 406\u001B[0m   func_graph \u001B[38;5;241m=\u001B[39m \u001B[43mfunction_def_lib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_def_to_graph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[43m      \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    408\u001B[0m \u001B[43m      \u001B[49m\u001B[43mstructured_input_signature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstructured_input_signature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    409\u001B[0m \u001B[43m      \u001B[49m\u001B[43mstructured_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstructured_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;66;03m# Restores gradients for function-call ops (not the same as ops that use\u001B[39;00m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;66;03m# custom gradients)\u001B[39;00m\n\u001B[1;32m    412\u001B[0m _restore_gradient_functions(func_graph, renamed_functions, loaded_gradients)\n",
      "File \u001B[0;32m~/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/tensorflow/python/framework/function_def_to_graph.py:63\u001B[0m, in \u001B[0;36mfunction_def_to_graph\u001B[0;34m(fdef, structured_input_signature, structured_outputs, input_shapes)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunction_def_to_graph\u001B[39m(fdef,\n\u001B[1;32m     35\u001B[0m                           structured_input_signature\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     36\u001B[0m                           structured_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     37\u001B[0m                           input_shapes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     38\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Converts a FunctionDef to a FuncGraph (sub-class Graph).\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m  The returned FuncGraph's `name`, `inputs` and `outputs` fields will be set.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;124;03m    A FuncGraph.\u001B[39;00m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m---> 63\u001B[0m   func_graph \u001B[38;5;241m=\u001B[39m \u001B[43mFuncGraph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfdef\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mstructured_input_signature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstructured_input_signature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mstructured_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstructured_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m input_shapes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     67\u001B[0m     input_shapes_attr \u001B[38;5;241m=\u001B[39m fdef\u001B[38;5;241m.\u001B[39mattr\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_input_shapes\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:195\u001B[0m, in \u001B[0;36mFuncGraph.__init__\u001B[0;34m(self, name, collections, capture_by_value, structured_input_signature, structured_outputs)\u001B[0m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, collections\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, capture_by_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    170\u001B[0m              structured_input_signature\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, structured_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    171\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Construct a new FuncGraph.\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \n\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03m  The graph will inherit its graph key, collections, seed, and distribution\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    193\u001B[0m \u001B[38;5;124;03m      information.\u001B[39;00m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 195\u001B[0m   \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mFuncGraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    196\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m name\n\u001B[1;32m    197\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/anaconda3/envs/tflite-model-maker/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:3088\u001B[0m, in \u001B[0;36mGraph.__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   3081\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Creates a new, empty Graph.\"\"\"\u001B[39;00m\n\u001B[1;32m   3082\u001B[0m \u001B[38;5;66;03m# Protects core state that can be returned via public accessors.\u001B[39;00m\n\u001B[1;32m   3083\u001B[0m \u001B[38;5;66;03m# Thread-safety is provided on a best-effort basis to support buggy\u001B[39;00m\n\u001B[1;32m   3084\u001B[0m \u001B[38;5;66;03m# programs, and is not guaranteed by the public `tf.Graph` API.\u001B[39;00m\n\u001B[1;32m   3085\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m   3086\u001B[0m \u001B[38;5;66;03m# NOTE(mrry): This does not protect the various stacks. A warning will\u001B[39;00m\n\u001B[1;32m   3087\u001B[0m \u001B[38;5;66;03m# be reported if these are used from multiple threads\u001B[39;00m\n\u001B[0;32m-> 3088\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock \u001B[38;5;241m=\u001B[39m \u001B[43mthreading\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRLock\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3089\u001B[0m \u001B[38;5;66;03m# The group lock synchronizes Session.run calls with methods that create\u001B[39;00m\n\u001B[1;32m   3090\u001B[0m \u001B[38;5;66;03m# and mutate ops (e.g. Graph.create_op()). This synchronization is\u001B[39;00m\n\u001B[1;32m   3091\u001B[0m \u001B[38;5;66;03m# necessary because it's illegal to modify an operation after it's been run.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3094\u001B[0m \u001B[38;5;66;03m# Similarly, if one or more Session.run calls are going on, all mutate ops\u001B[39;00m\n\u001B[1;32m   3095\u001B[0m \u001B[38;5;66;03m# have to wait until all Session.run calls have finished.\u001B[39;00m\n\u001B[1;32m   3096\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_group_lock \u001B[38;5;241m=\u001B[39m lock_util\u001B[38;5;241m.\u001B[39mGroupLock(num_groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/tflite-model-maker/lib/python3.9/threading.py:82\u001B[0m, in \u001B[0;36mRLock\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;66;03m# Synchronization classes\u001B[39;00m\n\u001B[1;32m     80\u001B[0m Lock \u001B[38;5;241m=\u001B[39m _allocate_lock\n\u001B[0;32m---> 82\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mRLock\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     83\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Factory function that returns a new reentrant lock.\u001B[39;00m\n\u001B[1;32m     84\u001B[0m \n\u001B[1;32m     85\u001B[0m \u001B[38;5;124;03m    A reentrant lock must be released by the thread that acquired it. Once a\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     89\u001B[0m \n\u001B[1;32m     90\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _CRLock \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_trace_dispatch_regular.py:434\u001B[0m, in \u001B[0;36mThreadTracer.__call__\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    431\u001B[0m     abs_path_real_path_and_base \u001B[38;5;241m=\u001B[39m get_abs_path_real_path_and_base_from_frame(frame)\n\u001B[1;32m    433\u001B[0m filename \u001B[38;5;241m=\u001B[39m abs_path_real_path_and_base[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m--> 434\u001B[0m file_type \u001B[38;5;241m=\u001B[39m \u001B[43mget_file_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43mabs_path_real_path_and_base\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# we don't want to debug threading or anything related to pydevd\u001B[39;00m\n\u001B[1;32m    436\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    437\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m file_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:  \u001B[38;5;66;03m# inlining LIB_FILE = 1\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for k,v in quantization_configs.items():\n",
    "    config_name = MODEL_NAME+\"_\"+k+\".tflite\"\n",
    "    model.export(export_dir='.',tflite_filename=config_name ,quantization_config=v)\n",
    "    res[k] = model.evaluate_tflite(config_name, test_data)\n",
    "    print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T13:10:54.502234980Z",
     "start_time": "2024-01-07T13:06:15.848191407Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-07T13:02:06.416589310Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me6_RwPZqNhX"
   },
   "source": [
    "## Test the TFLite model on your image\n",
    "\n",
    "Use the app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p79NHCx0xFqb"
   },
   "source": [
    "### Load the dataset\n",
    "\n",
    "#### Load your own data\n",
    "\n",
    "You can upload your own dataset to work through this tutorial. Upload your dataset by using the left sidebar in Colab.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_object_detection.png\" alt=\"Upload File\" width=\"1000\" hspace=\"0\">\n",
    "\n",
    "If you prefer not to upload your dataset to the cloud, you can also locally run the library by following the [guide](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker).\n",
    "\n",
    "#### Load your data with a different data format\n",
    "\n",
    "The Model Maker library also supports the `object_detector.DataLoader.from_pascal_voc` method to load data with [PASCAL VOC](https://towardsdatascience.com/coco-data-format-for-object-detection-a4c5eaf518c5#:~:text=Pascal%20VOC%20is%20an%20XML,for%20training%2C%20testing%20and%20validation) format. [makesense.ai](https://www.makesense.ai/) and [LabelImg](https://github.com/tzutalin/labelImg) are the tools that can annotate the image and save annotations as XML files in PASCAL VOC data format:\n",
    "```python\n",
    "object_detector.DataLoader.from_pascal_voc(image_dir, annotations_dir, label_map={1: \"person\", 2: \"notperson\"})\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8VxPiOLy4Gv"
   },
   "source": [
    "### Customize the EfficientDet model hyperparameters\n",
    "\n",
    "The model and training pipeline parameters you can adjust are:\n",
    "\n",
    "* `model_dir`: The location to save the model checkpoint files. If not set, a temporary directory will be used.\n",
    "* `steps_per_execution`: Number of steps per training execution.\n",
    "* `moving_average_decay`: Float. The decay to use for maintaining moving averages of the trained parameters.\n",
    "* `var_freeze_expr`: The regular expression to map the prefix name of variables to be frozen which means remaining the same during training. More specific, use `re.match(var_freeze_expr, variable_name)` in the codebase to map the variables to be frozen.\n",
    "* `tflite_max_detections`: integer, 25 by default. The max number of output detections in the TFLite model.\n",
    "* `strategy`:  A string specifying which distribution strategy to use. Accepted values are 'tpu', 'gpus', None. tpu' means to use TPUStrategy. 'gpus' mean to use MirroredStrategy for multi-gpus. If None, use TF default with OneDeviceStrategy.\n",
    "* `tpu`:  The Cloud TPU to use for training. This should be either the name used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\n",
    "* `use_xla`: Use XLA even if strategy is not tpu. If strategy is tpu, always use XLA, and this flag has no effect.\n",
    "* `profile`: Enable profile mode.\n",
    "* `debug`: Enable debug mode.\n",
    "\n",
    "Other parameters that can be adjusted is shown in [hparams_config.py](https://github.com/google/automl/blob/df451765d467c5ed78bbdfd632810bc1014b123e/efficientdet/hparams_config.py#L170).\n",
    "\n",
    "\n",
    "For instance, you can set the `var_freeze_expr='efficientnet'` which freezes the variables with name prefix `efficientnet` (default is `'(efficientnet|fpn_cells|resample_p6)'`). This allows the model to freeze untrainable variables and keep their value the same through training.\n",
    "\n",
    "```python\n",
    "spec = model_spec.get('efficientdet_lite0')\n",
    "spec.config.var_freeze_expr = 'efficientnet'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J2qre1fwXsi"
   },
   "source": [
    "### Change the Model Architecture\n",
    "\n",
    "You can change the model architecture by changing the `model_spec`. For instance, change the `model_spec` to the EfficientDet-Lite4 model.\n",
    "\n",
    "```python\n",
    "spec = model_spec.get('efficientdet_lite4')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvQuy7RSDir3"
   },
   "source": [
    "### Tune the training hyperparameters\n",
    "\n",
    "The `create` function is the driver function that the Model Maker library uses to create models. The `model_spec` parameter defines the model specification. The `object_detector.EfficientDetSpec` class is currently supported. The `create` function comprises of the following steps:\n",
    "\n",
    "1. Creates the model for the object detection according to `model_spec`.\n",
    "2. Trains the model.  The default epochs and the default batch size are set by the `epochs` and `batch_size` variables in the `model_spec` object.\n",
    "You can also tune the training hyperparameters like `epochs` and `batch_size` that affect the model accuracy. For instance,\n",
    "\n",
    "*   `epochs`: Integer, 50 by default. More epochs could achieve better accuracy, but may lead to overfitting.\n",
    "*   `batch_size`: Integer, 64 by default. The number of samples to use in one training step.\n",
    "*   `train_whole_model`: Boolean, False by default. If true, train the whole model. Otherwise, only train the layers that do not match `var_freeze_expr`.\n",
    "\n",
    "For example, you can train with less epochs and only the head layer. You can increase the number of epochs for better results.\n",
    "\n",
    "```python\n",
    "model = object_detector.create(train_data, model_spec=spec, epochs=10, validation_data=validation_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vPyZInPxJBT"
   },
   "source": [
    "### Export to different formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xqNIcBM-4YR"
   },
   "source": [
    "The export formats can be one or a list of the following:\n",
    "\n",
    "*   `ExportFormat.TFLITE`\n",
    "*   `ExportFormat.LABEL`\n",
    "*   `ExportFormat.SAVED_MODEL`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enhsZhW3ApcX"
   },
   "source": [
    "By default, it exports only the TensorFlow Lite model file containing the model [metadata](https://www.tensorflow.org/lite/models/convert/metadata) so that you can later use in an on-device ML application. The label file is embedded in metadata.\n",
    "\n",
    "In many on-device ML application, the model size is an important factor. Therefore, it is recommended that you quantize the model to make it smaller and potentially run faster. As for EfficientDet-Lite models, full integer quantization  is used to quantize the model by default. Please refer to [Post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) for more detail.\n",
    "\n",
    "```python\n",
    "model.export(export_dir='.')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLGZs6InAnP5"
   },
   "source": [
    "You can also choose to export other files related to the model for better examination. For instance, exporting both the saved model and the label file as follows:\n",
    "```python\n",
    "model.export(export_dir='.', export_format=[ExportFormat.SAVED_MODEL, ExportFormat.LABEL])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5q_McchQ2C4"
   },
   "source": [
    "### Customize Post-training quantization on the TensorFlow Lite model\n",
    "\n",
    "[Post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) is a conversion technique that can reduce model size and inference latency, while also improving CPU and hardware accelerator inference speed, with a little degradation in model accuracy. Thus, it's widely used to optimize the model.\n",
    "\n",
    "Model Maker library applies a default post-training quantization technique when exporting the model. If you want to customize post-training quantization, Model Maker supports multiple post-training quantization options using [QuantizationConfig](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/config/QuantizationConfig) as well. Let's take float16 quantization as an instance. First, define the quantization config.\n",
    "\n",
    "```python\n",
    "config = QuantizationConfig.for_float16()\n",
    "```\n",
    "\n",
    "\n",
    "Then we export the TensorFlow Lite model with such configuration.\n",
    "\n",
    "```python\n",
    "model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS4u77W5gnzQ"
   },
   "source": [
    "# Read more\n",
    "\n",
    "You can read our [object detection](https://www.tensorflow.org/lite/examples/object_detection/overview) example to learn technical details. For more information, please refer to:\n",
    "\n",
    "*   TensorFlow Lite Model Maker [guide](https://www.tensorflow.org/lite/models/modify/model_maker) and [API reference](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker).\n",
    "*   Task Library: [ObjectDetector](https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector) for deployment.\n",
    "*   The end-to-end reference apps: [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android), [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/ios), and [Raspberry PI](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model Maker Object Detection Tutorial",
   "provenance": [
    {
     "file_id": "1dbRXQCjtm-jBFC32DJ6YCVXnXBOG3M5t",
     "timestamp": 1613441434239
    },
    {
     "file_id": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb",
     "timestamp": 1612303859066
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
